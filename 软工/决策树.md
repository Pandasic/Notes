---
title: 决策树
tag: [计算机课程学习,软工,]
---
# 决策树

参考文章：[决策树(分类树、回归树）_你午睡了吗-CSDN博客](https://blog.csdn.net/weixin_36586536/article/details/80468426)

## 简述

用于分类与回归的一种树结构

- 内部节点：对应于一个属性测试
- 叶节点：对应于决策结果
- 根节点包含样本全集；
- 每个节点包括的样本集合根据属性测试的结果被划分到子节点中；
- 根节点到每个叶节点的路径对应对应了一个判定测试路径；

**决策树算法本质上就是要找出每一列的最佳划分以及不同列划分的先后顺序及排布**

## 信息论的基础知识

### 熵 

变量不确定的度量 X是一个取有限值的离散随机变量，其概率分布为
$$
P(X = x_i) = p_i\quad\quad i=1,2,...,n
$$
则X的熵为
$$
H(X) = -\sum^n_{i=1}pi\log_2 p_i
$$
熵越大不确定性越大



对于0-1 分布
$$
P(X = 1) = p,\quad P(X=0)=1-p\quad0\leq p \leq1
$$
熵为
$$
H(p) = -plog_2p-(1-p)log_2(1-p)
$$
![$H(p)$随概率p变化的曲线](https://img-blog.csdn.net/2018052612320692?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjU4NjUzNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 条件熵

条件熵H(Y|X)表示在一直随机变量X的条件下 随机Y的不确定性。
$$
H(Y \mid X)=\sum_{i=1}^{n} p_{i} H\left(Y \mid X=x_{i}\right)\\
p_i=P(X=x_i),i=1,2,...,n
$$

### 信息增益

表示已得知特征X 的信息是的类Y 的信息的不确定性减少程度

接下来给出定义，特征A对训练数据集D的信息增益g(D,A),为集合D的熵H(D)与特征A给定条件下D的条件熵H(D|A)之差
$$
g(D,A) = H(D) - H(D|A)
$$
信息增益的算法

(1):计算数据集D的熵H(D)
$$
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$


(2)计算特征A对训练数据集D的条件熵H(D|A)
$$
H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$


(3)计算信息增益
$$
g(D,A)=H(D)−H(D|A)
$$
|D|为样本容量，设有K个类Ck,k=1,2,...,K,|Ck|为属于类Ck的样本个数。设特征A有n个不同取值，根据特征A的取值将D划分为n个子集D1,D2,...,Dn,Dik为子集Di中属于类Ck的集合。

### 信息增益率

$$
g_R(D,A) = \frac{g(D,A)}{H_A(D)}\\
其中
H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}\quad n是特征，A为取值的个数
$$

 **信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。**

​    **缺点**：信息增益比偏向取值较少的特征  

​    **原因：** 当特征取值较少时HA(D)的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。

**使用信息增益比：基于以上缺点，**并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

### 基尼指数

**定义：**基尼指数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率。

**注意**： Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。

分类问题中，假设有K个类，样本点属于第k类的概率为pk则概率分布的基尼指数定义为
$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$
如果样本集合更具特征A是否取某一可能值a被分割为D1,D2两个布即
$$
D_{1}=\{(x, y) \in D \mid A(x)=a\}, \quad D_{2}=D-D_{1}
\\特征A的条件下集合D的基尼指数定义为
\\
\operatorname{Gini}(D, A=a)=\frac{\left|D_{1}\right|}{|D|} 
\operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
$$



## 决策树的学习算法

* 特征选择
* 决策树生成
* 决策树剪枝

### 特征选择

即选择最优的划分属性，从当前数据的特征中选择一个特征作为划分标准，决策树的分支节点包含样本尽可能一直的一类，纯度越来越高。

>  [了解信息论的基础知识](#信息论的基础知识)

## 决策树生成

| 生成算法 | 划分标准   |
| -------- | ---------- |
| ID3      | 信息增益   |
| C4.5     | [信息增益率](#信息增益率) |
| CART     | 基尼指数   |

### ID3

ID3算法的核心是在决策树各个节点上根据[信息增益](#信息增益)来选择进行划分的特征，然后递归地构建决策树。

#### 具体方法：

1. 从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益值最大的特征作为节点的划分特征；
2. 由该特征的不同取值建立子节点；
3. 再对子节点递归地调用以上方法，构建决策树；
4. 到所有特征的信息增益都很小或者没有特征可以选择为止，得到最终的决策树

#### ID3的局限：

没有剪枝
采用信息增益作为选择最优划分特征的标准，然而**信息增益会偏向那些取值较多的特征**(这也是C4.5采用[信息增益率](#信息增益率)的原因)

### C4.5:
C4.5与ID3相似，但对ID3进行了改进，在这里不再详细描述C4.5的实现，就讲一下有哪些基ID3的改进：

* 用[信息增益率](#信息增益率)来选择划分特征，克服了用信息增益选择的不足
* 在构造树的过程中进行剪枝
* 可对连续值与缺失值进行处理

#### C4.5 选取[信息增益率](#信息增益率)的原因

**为什么C4.5要用[信息增益率](#信息增益率)代替信息增益？为什么信息增益会偏向多取值特征？**
信息增益在面对类别较少的离散数据时效果较好，但是面对取值较多的特征时效果会很不如人意。关于信息增益对取值较多特征的偏向性，我认为原因是：当特征的取值较多时，根据此特征划分得到的子集纯度有更大的可能性会更高(对比与取值较少的特征)，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较偏向取值较多的特征。

举个较为极端的例子可能更好理解：如果特征A的取值能将每一个样本都分到一个节点当中去的时候(如编号等特征)，条件熵部分会为0，这意味着该情况下的信息增益达到了最大值，故ID3算法一定会选择特征A。

但是，显然的，我们知道这个特征A显然不是最佳选择。

那么为什么[信息增益率](#信息增益率)就能改善这种情况呢？先来看一下[信息增益率](#信息增益率)的计算公式：
$$
g_R(D,A) = \frac{g(D,A)}{H_A(D)}\\

其中\\H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|},\\
$$
其中`H_A(D)`又叫做特征A的内部信息，H_A(D)其实像是一个衡量以特征A的不同取值将数据集D分类后的不确定性的度量。如果特征AA的取值越多，那么不确定性通常会更大，那么H_A(D)的值也会越大，而1/HA(D)的值也会越小。这相当于是在信息增益的基础上乘上了一个惩罚系数。
$$
g_R(D,A)=g(D,A)∗惩罚系数
$$

### CART
CART(classificationandregressiontree)分类回归树算法，既可用于**分类也可用于回归** ，在这一部分我们先主要将其分类树的生成。

区别于ID3和C4.5,CART假设决策树是**二叉树**，内部节点特征的取值为“是”和“否”，左分支为取值为“是”的分支，右分支为取值为”否“的分支。这样的决策树等价于递归地二分每个特征，将输入空间(即特征空间)划分为有限个单元。

CART的分类树用基尼指数来选择最优特征的最优划分点，具体过程如下

1. 从根节点开始，对节点计算现有特征的基尼指数，对每一个特征，例如AA，再对其每个可能的取值如a,根据样本点对A=a的结果的”是“与”否“划分为两个部分，利用
   $$
   \operatorname{Gini}(D, A=a)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gin} i\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
   $$
    进行计算；

2. 在所有可能的特征AA以及该特征所有的可能取值a中，选择基尼指数最小的特征及其对应的取值作为最优特征和最优切分点。然后根据最优特征和最优切分点，将本节点的数据集二分，生成两个子节点

3. 对两个字节点递归地调用上述步骤，直至节点中的样本个数小于阈值，或者样本集的基尼指数小于阈值，或者没有更多特征后停止；

4. 生成CART分类树；



