# Linux 内存模型与杂项

## Linux 虚拟地址空间

为了防止不同进程同一时刻在内存中运行而对物理内存的争夺和踩踏 采用虚拟内存

> 虚拟内存技术使得不同进程在运行过程中,它所看到的是自己独自占有了当前系统的 4G 内存。所有进程共享同一物理内存,每个进程只把自己目前需要的虚拟内存空间映射并存储到物理 内存上。 事实上,在每个进程创建加载时,内核只是为进程"创建"了虚拟内存的布局,具体 就是初始化进程控制表中内存相关的链表,实际上并不立即就把虚拟内存对应位置的程序数据和 代码(比如.text .data 段)拷贝到物理内存中,只是建立好虚拟内存和磁盘文件之间的映射就 好(叫做存储器映射),等到运行到对应的程序时,才会通过缺页异常,来拷贝数据。还有进程 运行过程中,要动态分配内存,比如 malloc 时,也只是分配了虚拟内存,即为这块虚拟内存对 应的页表项做相应设置,当进程真正访问到此数据时,才引发缺页异常。请求分页系统.请求分段系统和请求段页式系统都是针对虚拟内存的,通过请求实现内存与 外存的信息置换。

* 虚拟内存的好处: 
  * 扩大地址空间; 
   * 内存保护:每个进程运行在各自的虚拟内存地址空间,互相不能干扰对方。虚存还对特定
     的内存地址提供写保护,可以防止代码或数据被恶意篡改。 
   * 公平内存分配。采用了虚存之后,每个进程都相当于有同样大小的虚存空间。 
   * 当进程通信时,可采用虚存共享的方式实现。 
   * 当不同的进程使用同样的代码时,比如库文件中的代码,物理内存中可以只存储一份这样
     的代码,不同的进程只需要把自己的虚拟内存映射过去就可以了,节省内存 
   * 虚拟内存很适合在多道程序设计系统中使用,许多程序的片段同时保存在内存中。当一个
     程序等待它的一部分读入内存时,可以把 CPU 交给另一个进程使用。在内存中可以保留多个进程, 系统并发度提高
   * 在程序需要分配连续的内存空间的时候,只需要在虚拟内存空间分配连续空间,而不需要 实际物理内存的连续空间,可以利用碎片

* 虚拟内存的代价: 
  * 虚存的管理需要建立很多数据结构,这些数据结构要占用额外的内存 
  * 虚拟地址到物理地址的转换,增加了指令的执行时间。 
  * 页面的换入换出需要磁盘 I/O,这是很耗时的 
  * 如果一页中只有一部分数据,会浪费内存。

## 操作系统 内存结构

![image-20200708193022180](..\imgs\C++面试_7_2.jpg)

* 一个程序本质上都是由 BSS 段.data 段.text 段三个组成的。
* 一个可执行程序在存储(没有调入内存)时分为代码段.数据区和未初始化数据区三部分。
* BSS 段(未初始化数据区):通常用来存放程序中未初始化的全局变量和静态变量的一块内
  存区域。BSS 段属于静态分配,程序结束后静态变量资源由系统自动释放。 数据段:存放程序中已初始化的全局变量的一块内存区域。数据段也属于静态内存分配
* 代码段:存放程序执行代码的一块内存区域。这部分区域的大小在程序运行前就已经确定,
  并且内存区域属于只读。在代码段中,也有可能包含一些只读的常数变量 text 段和 data 段在编译时已经分配了空间,而 BSS 段并不占用可执行文件的大小,它是由链接器来获取内存的。 
* bss 段(未进行初始化的数据)的内容并不存放在磁盘上的程序文件中。其原因是内核在程
  序开始运行前将它们设置为 0。需要存放在程序文件中的只有正文段和初始化数据段。 
* data 段(已经初始化的数据)则为数据分配空间,数据保存到目标文件中。 
* 数据段包含经过初始化的全局变量以及它们的值。BSS 段的大小从可执行文件中得到,然后链接器得到这个大小的内存块,紧跟在数据段的后面。当这个内存进入程序的地址空间后全部清 零。包含数据段和 BSS 段的整个区段此时通常称为数据区。
* 可执行程序在运行时又多出两个区域:栈区和堆区。 
* **栈区**:由**编译器自动释**放,存放函数的参数值.局部变量等。每当一个函数被调用时,该
  函数的返回类型和一些调用的信息被存放到栈中。然后这个被调用的 函数再为他的自动变量和 临时变量在栈上分配空间。每调用一个函数一个新的栈就会被使用。栈区是从高地址位向低地 址位增长的,是一块连续的内存区域,最大容 量是由系统预先定义好的,申请的栈空间超过这 个界限时会提示溢出,用户能从栈中获取的空间较小。
* **堆区**:用于动态分配内存,位于 BSS 和栈中间的地址区域。由程序员申请分配和释放。堆
  是从低地址位向高地址位增长,采用链式存储结构。频繁的 malloc/free 造成内存空间的不连 续,产生碎片。当申请堆空间时库函数是按照一定的算法搜索可用的足够大的空间。因此堆的效 率比栈要低的多。

### C/C++内存分配

* 32BItCPU 可寻址4GB空间 每个进程拥有独立的4G逻辑地址 0-3式用户态空间 3-4G 式内核空间
* 静态区域: 
  * text segment(代码段):包括只读存储区和文本区,其中只读存储区存储字符串常量,文本区存储程序的机器代码。 
  * data segment(数据段):存储程序中已初始化的全局变量和静态变量 
  * bss segment:存储未初始化的全局变量和静态变量(局部+全局),以及所有被初始化为 0的全局变量和静态变量,对于未初始化的全局变量和静态变量,程序运行 main 之前时会统一清 零。即未初始化的全局变量编译器会初始化为 0
* 动态区域: 
  * heap(堆): 当进程未调用 malloc 时是没有堆段的,只有调用 malloc 时采用分配一个堆,并且在程序运行过程中可以动态增加堆大小(移动 break 指针),从低地址向高地址增长。分配小 内存时使用该区域。 堆的起始地址由 mm_struct 结构体中的 start_brk 标识,结束地址由 brk 标识。
  * memory mapping segment(映射区):存储动态链接库等文件映射.申请大内存(malloc 时调用 mmap 函数) 
  * stack(栈):使用栈空间存储函数的返回地址.参数.局部变量.返回值,从高地址向低地址增长。在创建进程时会有一个最大栈大小,Linux 可以通过 ulimit 命令指定。****

### 例

#### A *a = new A; a->i = 10 在内核中内存分配发生了什么?

[内存分配](#操作系统 内存结构)

```C++
A* a = new A;
a->i = 10: 
```

1. A *a:a 是一个局部变量,类型为指针,故而操作系统在程序栈区开辟 4/8 字节的空间(0x000m),分配给指针 a。*
2. new A:通过 new 动态的在堆区申请类 A 大小的空间(0x000n)。 
3. a = new A:将指针 a 的内存区域填入栈中类 A 申请到的地址的地址。即*(0x000m)=0x000n。 *
4. *a->i:先找到指针 a 的地址 0x000m,通过 a 的值 0x000n 和 i 在类 a 中偏移 offset,得到 a->i 的地址 0x000n + offset,进行*(0x000n + offset) = 10 的赋值操作,即内存 0x000n + offset 的值是 10。

#### 类里有static和virtual 描述类的内存分布

* static

  * static 修饰成员变量

    静态数据成员被当作是类的成员。只有一个拷贝。被所有对象共享【包括派生类】

  * static 修饰成员函数

    * 不和任何对象联系 不具有this指针。违法访问类对象的非静态成员与函数。
    * static 修饰的成员函数再代码区分配内存

* C++继承和虚函数

  * 静态多态
    * 静态多态通过模板和模板技术实现 在编译的时候确定。
  * 动态多态
    * 动态多态通过虚函数和继承关系来实现,执行动态绑定,在运行的时候确定
  * 虚函数
    * 实现条件:虚函数,基类指针指向派生的对象
    * 基类指针调用成员函数的时候 会寻找改对象的虚函数表。
    * 虚函数表在每个对象的首地址。查找改函数的指针进行调用。
    * 每个对象保存的只是一个虚函数表的指针 C++内部为每个类维持一个虚函数表 该类的对象指向同一个虚函数表
    * 虚函数表中为什么就能准确查找相应的函数指针呢?因为在类设计的时候,虚函数表直接从基类也继承过来,如果覆盖了其中的某个虚函数,那么虚函数表的指针就会被替 换,因此可以根据指针准确找到该调用哪个函数。

* virtual修饰符

  * 如果一个类是局部变量则该类数据存储在栈区,如果一个类是通过new/malloc动态申请的,则该类数据存储在堆区。 

  * 如果该类是 virutal 继承而来的子类,则该类的虚函数表指针和该类其他成员一起存储。虚函数表指针指向只读数据段中的类虚函数表,虚函数表中存放着一个个函数指针,函数指针指 向代码段中的具体函数。

  * 如果类中成员是 virtual 属性,会隐藏父类对应的属性。

    ![image-20200714153018113](E:\Notes\MarkDown\计算机课程学习\imgs\C++面试_7_8.jpg)

  

## 缺页中断

malloc()和 mmap()等内存分配函数,在分配时只是建立了进程虚拟地址空间,并没有分配
虚拟内存对应的物理内存。当进程访问这些没有建立映射关系的虚拟内存时,处理器自动触发一 个缺页异常。
缺页中断:在请求分页系统中,可以通过查询页表中的状态位来确定所要访问的页面是否存
在于内存中。每当所要访问的页面不在内存是,会产生一次缺页中断,此时操作系统会根据页表 中的外存地址在外存中找到所缺的一页,将其调入内存。
缺页本身是一种中断,与一般的中断一样,需要经过 4 个处理步骤:

1. 保护 CPU 现场

2. 分析中断原因
3. 转入缺页中断处理程序进行处理 
4. 恢复 CPU 现场,继续执行

但是缺页中断是由于所要访问的页面不存在于内存时,由硬件所产生的一种特殊的中断,因
此,与一般的中断存在区别: 

* 在指令执行期间产生和处理缺页中断信号 
* 一条指令在执行期间,可能产生多次缺页中断 
* 缺页中断返回是,执行产生中断的一条指令,而一般的中断返回是,执行下一条指令。

## MYSQL 端口号

使用 show global variables like 'port':查看端口号

| 数据库引擎 | 默认端口号 |
| ---------- | ---------- |
| MySQL      | 3306       |
| SQLServer  | 1433       |
| Oracle     | 1521       |
| DB2        | 5000       |
| PostgreSQL | 5432       |

修改:编辑/etc/my.cnf增加端口参数

## 页表寻址

​    页式内存管理,内存分成固定长度的一个个页片。操作系统为每一个进程维护了一个从虚拟地址到物理地址的映射关系的数据结构,叫页表,页表的内容就是该进程的虚拟地址到物理地址 的一个映射。页表中的每一项都记录了这个页的基地址。通过页表,由逻辑地址的高位部分先找 到逻辑地址对应的页基地址,再由页基地址偏移一定长度就得到最后的物理地址,偏移的长度由 逻辑地址的低位部分决定。一般情况下,这个过程都可以由硬件完成,所以效率还是比较高的。 页式内存管理的优点就是比较灵活,内存管理以较小的页为单位,方便内存换入换出和扩充地址 空间。

### Linux 最初的二级页表机制

* 两级分页机制将 32 位的虚拟空间分成三段,低十二位表示页内偏移,高 20 分成两段分别表示两级页表的偏移。 

  * PGD(Page Global Directory): 最高 10 位 全局页目录表索引 
  * PTE(Page Table Entry):中间 10 位,页表入口索引 

* 当在进行地址转换时,结合在 CR3 寄存器中存放的**页目录**(page directory, **PGD**)的这一页的物理地址,再加上从虚拟地址中抽出高 10 位叫做页目录表项(内核也称这为 pgd)的部分作为 偏移, 即定位到可以描述该地址的pgd;从该pgd中可以获取可以描述该地址的页表的物理地址, 再加上从虚拟地址中抽取中间 10 位作为偏移, 即定位到可以描述该地址的 pte;在这个 pte 中 即可获取该地址对应的页的物理地址, 加上从虚拟地址中抽取的最后 12 位,即形成该页的页内偏移, 即可最终完成从虚拟地址到物理地址的转换。从上述过程中,可以看出,对虚拟地址的分 级解析过程,实际上就是不断深入页表层次,逐渐定位到最终地址的过程,所以这一过程被叫做 page talbe walk。

  ```flow
  st=>start: 开始寻址
  PGD=>operation: PGD
  [CR3页目录]
  [虚拟地址(高10位)页目录表项]
  
  PTE=>operation: PTE
  [对应的页的物理地址]
  [虚拟地址最后12位]
  即形成该页的页内偏移
  
  fl=>operation: 寻找最合适的list
  isempty=>condition: 链表是否为空
  rn=>operation: 返回第一个node链表头 改
  st()->PGD->PTE
  
  ```

### Linux 的三级页表机制

当 X86 引入物理地址扩展(Pisycal Addrress Extension, PAE)后,可以支持大于 4G 的物理内存(36位),但虚拟地址依然是32位,原先的页表项不适用,它实际多4 bytes被扩充到8 bytes, 这意味着,每一页现在能存放的 pte 数目从 1024 变成 512 ,(4k/8)。相应地,页表层级发生了 变化,

Linus 新增加了一个层级,叫做页中间目录(page middle directory, PMD), 变成:

| 字段        | 描述                          | 位数            |
| ----------- | ----------------------------- | --------------- |
| CR3         | 指向一个PDPT                  | crs寄存器出储存 |
| PGD         | 指向PDPT 中4个项中的一个位    | 31-30           |
| PMD         | 指向页目录中 512 项中的一个位 | 29-21           |
| PTE         | 指向页目录中 512 项中的一个位 | 20-12           |
| page offset | 4KB 页中的偏移                | 11-0            |

现在就同时存在 2 级页表和 3 级页表,在代码管理上肯定不方便。巧妙的是,Linux 采取了
一种抽象方法:所有架构全部使用 3 级页表: 即 PGD -> PMD -> PTE。那只使用 2 级页表(如非 PAE 的 X86)怎么办?
办法是针对使用 2 级页表的架构,把 PMD 抽象掉,即虚设一个 PMD 表项。这样在 page table
walk 过程中,PGD 本直接指向 PTE 的,现在不了,指向一个虚拟的 PMD,然后再由 PMD 指向 PTE。 这种抽象保持了代码结构的统一。

```flow
st=>start: 开始寻址
PGD=>operation: PGD
[CR3页目录]
[虚拟地址(高10位)页目录表项]

PMD=>operation: PMD 
页中间目录

PTE=>operation: PTE
[对应的页的物理地址]
[虚拟地址最后12位]
即形成该页的页内偏移

fl=>operation: 寻找最合适的list
isempty=>condition: 链表是否为空
rn=>operation: 返回第一个node链表头 改
st()->PGD->PMD->PTE
```

### Linux 的四级页表机制

硬件在发展,3 级页表很快又捉襟见肘了,原因是 64 位 CPU 出现了, 比如 X86_64, 它的硬
件是实实在在支持 4 级页表的。它支持 48 位的虚拟地址空间 1。如下: 

| 字段        | 描述                          | 位数  |
| ----------- | ----------------------------- | ----- |
| PML4        | 指向一个PDPT                  | 47-39 |
| PGD         | 指向PDPT 中4个项中的一个位    | 38-30 |
| PMD         | 指向页目录中 512 项中的一个位 | 29-21 |
| PTE         | 指向页目录中 512 项中的一个位 | 20-12 |
| page offset | 4KB 页中的偏移                | 11-0  |

Linux 内核针为使用原来的 3 级列表(PGD->PMD->PTE),做了折衷。即采用一个唯一的,共
享的顶级层次,叫 PML4。

> 这个 PML4 没有编码在地址中,这样就能套用原来的 3 级列表方案了。 不过代价就是,由于只有唯一的 PML4, 寻址空间被局限在(239=)512G, 而本来 PML4 段有 9 位, 可以支持 512 个 PML4 表项的。现在为了使用 3 级列表方案,只能限制使用一个, 512G 的空间 很快就又不够用了,解决方案呼之欲出。
>
> 在 2004年 10月,当时的X86_64架构代码的维护者 Andi Kleen提交了一个叫做 4level page
> tables for Linux 的 PATCH 系列,为 Linux 内核带来了 4 级页表的支持。在他的解决方案中, 不出意料地,按照 X86_64 规范,新增了一个 PML4 的层级, 在这种解决方案中,X86_64 拥一个 有 512 条目的 PML4, 512 条目的 PGD, 512 条目的 PMD, 512 条目的 PTE。对于仍使用 3 级目录的 架构来说,它们依然拥有一个虚拟的 PML4,相关的代码会在编译时被优化掉。 这样,就把 Linux 内核的 3 级列表扩充为 4 级列表。这系列 PATCH 工作得不错,不久被纳入 Andrew Morton 的-mm 树接受测试。不出意外的话,它将在 v2.6.11 版本中释出。但是,另一个知名开发者 Nick Piggin 提出了一些看法,他认为 Andi 的 Patch 很不错,不过他认为最好还是把 PGD 作为第一级目录, 把新增加的层次放在中间,并给出了他自己的 Patch:alternate 4-level page tables patches。 Andi 更想保持自己的 PATCH, 他认为 Nick 不过是玩了改名的游戏,而且他的 PATCH 经过测试很 稳定,快被合并到主线了,不宜再折腾。不过 Linus 却表达了对 Nick Piggin 的支持,理由是 Nick 的做法 conceptually least intrusive。毕竟作为 Linux 的扛把子,稳定对于 Linus 来说 意义重大。
>
> 最终,不意外地,最后 Nick Piggin 的 PATCH 在 v2.6.11 版本中被合并入主线。在这 种方案中,4 级页表分别是:PGD -> PUD -> PMD -> PTE。

## OS 缺页置换算法

当访问一个内存中不存在的页,并且内存已满,则需要从内存中调出一个页或将数据送至磁
盘对换区,替换一个页,这种现象叫做缺页置换。

当前操作系统最常采用的缺页置换算法如下: 

* 先进先出(FIFO)算法:置换最先调入内存的页面,即置换在内存中驻留时间最久的页面。按照进入内存的先后次序排列成队列,从队尾进入,从队首删除。 

* 最近最少使用(LRU)算法: 置换最近一段时间以来最长时间未访问过的页面。根据程序局
  部性原理,刚被访问的页面,可能马上又要被访问;而较长时间内没有被访问的页面,可能最近 不会被访问。

  当前最常采用的就是 LRU 算法。

## 虚拟内存和物理内存

* 物理地址

  基于芯片内存级的但愿寻址.处理器和CPU连接的地址总线相对应。

  硬件提供给软件的抽象

* 虚拟地址
  * 对整个内存的抽象描述。并不与物理地址上的大地址相对应
  * 是因为现代操作系统都提供了一种内存管理的抽像,即虚拟内存(virtual memory)。进程使用虚拟内存中的地址,由操作系统协助相关硬件,把它"转换"成真正的物理 地址。
  
* 地址转换

  1. CPU段式管理--逻辑地址转线性地址

     CPU 要利用段时内存管理单元。先将一个逻地址转换成一个线程地址。

     逻辑地址包括两个部分组成:

     段标识符【段内偏移量】=>16位场的字段组成。前13位为索引 后3为为硬件细节。

     ![image-20200712141655548](..\imgs\C++面试_7_1.jpg)

  通过段标识符中的索引号从GDT.LDT 找到该段的段描述符 base字段是段的起始地址:

  ​	段描述符:base字段,描述了一个短开始位置的线性地址。

  	*  一些全局的段描述符,就放在"全局段描述符表(GDT)"中, 	.
   *   一些局部的,例如每个进程自己的,就放在所谓的"局部段描述符表(LDT)"中。
   *  GDT 在内存中的地址和大小存放在 CPU 的 gdtr 控制寄存器中,而 LDT 则在 ldtr 寄存器中。 
       *  段起始地址 + 段内偏移量 = 线性地址

![image-20200712142147502](..\imgs\C++面试_7_3.jpg)

首先,给定一个完整的逻辑地址[段选择符:段内偏移地址], 

1. 看段选择符的 T1=0 还是 1,知道当前要转换是 GDT 中的段,还是 LDT 中的段,再根据相应寄存器,得到其地址和大小。我们就有了一个数组了。

2. 拿出段选择符中前 13 位,可以在这个数组中,查找到对应的段描述符,这样,它了 Base,即基地址就知道了。 

3. 把 Base + offset,就是要转换的线性地址了。

![image-20200712142703892](..\imgs\C++面试_7_4.jpg)

页式管理——线性地址转物理地址 再利用其页式内存管理单元,转换为最终物理地址。

> linux 假的段式管理 Intel 要求两次转换,这样虽说是兼容了,但是却是很冗余,但是这是 intel 硬件的要求。 其它某些硬件平台,没有二次转换的概念,Linux 也需要提供一个高层抽像,来提供一个统 一的界面。 所以,Linux 的段式管理,事实上只是"哄骗"了一下硬件而已。 按照 Intel 的本意,全局的用 GDT,每个进程自己的用 LDT——不过 Linux 则对所有的进程都使用了相同的段来对指令和数据寻址。即用户数据段,用户代码段,对应的,内核中的是内核 数据段和内核代码段。
 在 Linux 下,逻辑地址与线性地址总是一致的,即逻辑地址的偏移量字段的值与线性地址的 值总是相同的。 linux 页式管理 CPU 的页式内存管理单元,负责把一个线性地址,最终翻译为一个物理地址。 

线性地址被分为以固定长度为单位的组,称为页(page),例如一个 32 位的机器,线性地址最大可为4G,可以用4KB为一个页来划分,这页,整个线性地址就被划分为一个tatol_page[2^20] 的大数组,共有 2 的 20 个次方个页。另一类"页",我们称之为物理页,或者是页框.页桢的。是分页单元把所有的物理内存也 划分为固定长度的管理单位,它的长度一般与内存页是一一对应的。

![image-20200712142805074](..\imgs\C++面试_7_5.jpg)

每个进程都有自己的页目录,当进程处于运行态的时候,其页目录地址存放在 cr3 寄存器中。

每一个 32 位的线性地址被划分为三部份,

​	页目录索引(10 位):页表索引(10 位):页内偏 移(12 位)

根据线性地址前十位,在数组中,找到对应的索引项,因为引入了二级管理模式,页目录中的项,不再是页的地址,而是一个页表的地址。(又引入了一个数组),页的地址被放到页表中 去了。
根据线性地址的中间十位,在页表(也是数组)中找到页的起始地址;将页的起始地址与线性地址中最后 12 位相加。

目的: 

内存节约:如果一级页表中的一个页表条目为空,那么那所指的二级页表就根本不会存在。这表现出一种巨大的潜在节约,因为对于一个典型的程序,4GB 虚拟地址空间的大部份都会是未 分配的;

![image-20200712144319597](E:\Notes\MarkDown\计算机课程学习\imgs\C++面试_7_6.jpg)

32 位,PGD = 10bit,PUD = PMD = 0,table = 10bit,offset = 12bit 64 位,PUD 和 PMD ≠ 0



## 操作系统结构体对齐 字节对齐

* 原因:
  * 平台:

    不是所有的硬件平台都能访问任意地址上的任意数据的;某些硬件平台只能在某些地址处取某些特定类型的数据,否则抛出硬件异常。

  * 性能原因:

    数据结构(尤其是栈)应该尽可能地在自然边界上对齐。原因在于,为了访问未对齐的内存,处理器需要作两次内存访问;而对齐的内存访问仅需要一次访问。

* 规则

  * 数据成员对齐规则:结构(struct)(或联合(union))的数据成员,第一个数据成员放在offset 为 0 的地方,以后每个数据成员的对齐按照#pragma pack 指定的数值和这个数据成员自 身长度中,比较小的那个进行。
  * 结构(或联合)的整体对齐规则:在数据成员完成各自对齐之后,结构(或联合)本身也要进行对齐,对齐将按照#pragma pack 指定的数值和结构(或联合)最大数据成员长度中,比较小 的那个进行。
  * 结构体作为成员:如果一个结构里有某些结构体成员,则结构体成员要从其内部最大元素大小的整数倍地址开始存储。

* 定义结构体对齐 
  
  * 可以通过预编译命令**#pragma pack(n)**,n=1,2,4,8,16 来改变这一系数,其中的 n 就是指定的"对齐系数"。

```C++
#pragma pack(2) 
struct AA { 
    int a;//长度 4 > 2 按 2 对齐;偏移量为 0;存放位置区间[0,3]
	char b; //长度 1 < 2 按 1 对齐;偏移量为 4;存放位置区间[4] 
    short c;//长度 2 = 2 按 2 对齐;偏移量要提升到 2 的倍数 6;存放位置区间[6,7] 
    char d; //长度 1 < 2 按 1 对齐;偏移量为 7;存放位置区间[8];共九个字节
}; 
#pragma pack()
```

## 虚拟内存置换方式

* FIFO(先进先出淘汰算法) 
  * 思想:最近刚访问的,将来访问的可能性比较大。 
  * 实现:使用一个**队列**,新加入的页面放入队尾,每次淘汰队首的页面,即最先进入的数据,最先被淘汰。 
  * 弊端:无法体现页面冷热信息 
* LFU(最不经常访问淘汰算法) 
  * 思想:如果数据过去被访问多次,那么将来被访问的频率也更高。 
  * 实现:每个数据块一个**引用计数**,所有数据块按照引用计数排序,具有相同引用计数的数据块则按照时间排序。每次淘汰队尾数据块。 
  * 开销:排序开销。
  * 弊端:缓存颠簸。
* LRU(最近最少使用替换算法)
  * 思想:如果数据最近被访问过,那么将来被访问的几率也更高。
  * 实现:使用一个**栈**,新页面或者命中的页面则将该页面移动到栈底,每次替换栈顶的缓存页面。
  * 优点:LRU 算法对热点数据命中率是很高的。
  * 缺陷:
    * 缓存颠簸,当缓存(1,2,3)满了,之后数据访问(0,3,2,1,0,3,2,1。。。)。
    * 缓存污染,突然大量偶发性的数据访问,会让内存中存放大量冷数据。
* LRU-K(LRU-2.LRU-3)
  * 思想:最久未使用 K 次淘汰算法。LRU-K 中的 K 代表最近使用的次数,因此 LRU 可以认为是 LRU-1。LRU-K 的主要目的是为了 解决 LRU 算法"**缓存污染**"的问题,其核心思想是将"最近使用过 1 次"的判断标准扩展为"最
    近使用过 K 次"。相比 LRU,LRU-K 需要多维护一个队列,用于记录所有缓存数据被访问的历史。只有当数据
    的访问次数达到 K 次的时候,才将数据放入缓存。当需要淘汰数据时,LRU-K 会淘汰第 K 次访问
    时间距当前时间最大的数据。
  * 实现:
    * 数据第一次被访问,加入到访问历史列表;
    * 如果数据在访问历史列表里后没有达到 K 次访问,则按照一定规则(FIFO,LRU)淘汰;
    * 当访问历史队列中的数据访问次数达到 K 次后,将数据索引从历史队列删除,将数据移到缓存队列中,并缓存此数据,缓存队列重新按照时间排序;
    * 缓存数据队列中被再次访问后,重新排序;
    * 需要淘汰数据时,淘汰缓存队列中排在末尾的数据,即:淘汰"倒数第 K 次访问离现在最久"的数据。 
  * 针对问题: 
    * LRU-K 的主要目的是为了解决 LRU 算法"缓存污染"的问题,其核心思想是将"最近使用过1 次"的判断标准扩展为"最近使用过 K 次"。 
* 2Q 类似 LRU-2。使用一个 FIFO 队列和一个 LRU 队列。 
  * 实现:
    * 新访问的数据插入到 FIFO 队列;
    * 如果数据在 FIFO 队列中一直没有被再次访问,则最终按照 FIFO 规则淘汰;
    * 如果数据在 FIFO 队列中被再次访问,则将数据移到 LRU 队列头部;
    * 如果数据在 LRU 队列再次被访问,则将数据移到 LRU 队列头部;
    * LRU 队列淘汰末尾的数据。 
  * 针对问题:LRU 的缓存污染 
  * 弊端: 当 FIFO 容量为 2 时,访问负载是:ABCABCABC 会退化为 FIFO,用不到 LRU。

## 硬链接和软链接

* 好处:隐藏文件路径.增加权限安全及节省存储等好处

* 硬链接:同一个问价使用了不同的别名 使用ln创建
* 文件用户数据块中存放的内容是另一个文件的路径名指向 则该文件为软连接。软链接是个普通文件 有自己独立inode 文件内容比较特殊

## 大端 小端

[大端 小端](https://blog.csdn.net/qqliyunpeng/article/details/68484497) [联合体union 一个union代表多个类型 共享内存0](https://blog.csdn.net/u013066730/article/details/84638489)

* 大端是指低字节存储在高地址

* 小端存储是指低字节存储在低地址。
* 我们可以根据联合体来判断该系统是大端还是小端。因为联合体变量总是从低地址存储。

```C++
int fun()
{
    union test{int i;char c;}
    test t;t.i = 1;
    //如果是大端t.c 为0x00 t.c!=1 返回0
    //	  是小端他t.c的值为1 返回1
    return t.c==1;
}
```

## 静态变量 初始化

静态变量 储存在虚拟地址空间的数据段和BSS段

* C语言在代码执行前初始化 属于编译器初始化
* C++中由于引入对象,对象生成必须调用构造函数,因此 C++规定全局或局部 静态对象当且仅当对象首次用到时进行构造

## 用户态和内核态

操作系统的两种运行级别。

用户态拥有最低的特权级。运行在用户态的程序不能直接访问操作系统内核数据 结构和程序。

内核态拥有最高的特权级。

内核态和用户态之间的转换方式主要包括:系统调用,异常和中断。

## 内存溢出和内存泄漏

### 内存溢出

程序申请内存的时候 没有足够的内存供申请者使用。

内存溢出就是你要的内存空间超过了系统实 际分配给你的空间,此时系统相当于没法满足你的需求,就会报内存溢出的错误

产生的原因:

* 内存中加载的数据量过于庞大,如一次从数据库取出过多数据 
* 集合类中有对对象的引用,使用完后未清空,使得不能回收 
* 代码中存在死循环或循环产生过多重复的对象实体 
* 使用的第三方软件中的 BUG 
* 启动参数内存值设定的过小

### 内存泄漏

* 内存泄漏通常是由于调用了malloc/new等内存申请的操作,但是缺少了对应free/delete。为了**判断**内存是否泄露。
* 使用 linux 环境下的内存泄漏检查工具 Valgrind
* 我们在写代码时可以添加内存申请和释放的统计功能,统计当前申请和释放的内存是否一致, 以此来判断内存是否泄露。
* 分类
  * 堆内存泄漏:对内存指的是程序运行中根据需要分配通过 malloc,realloc,new 等从堆中分配的一块内存,再是完成后必须通过调用对应的 free 或者 delete 删掉。如果 程序的设计的错误导致这部分内存没有被释放,那么此后这块内存将不会被使用,就会产生 Heap Leak.
  * 系统资源泄漏:要指程序使用系统分配的资源比如Bitmap,handle ,SOCKET 等没有使用相应的函数释放掉,导致系统资源的浪费,严重可导致系统 效能降低,系统运行不稳定。
  * 没有将基类的析构函数定义为虚函数。当基类指针指向子类对象时,如果基类的析构函
    数不是 virtual,那么子类的析构函数将不会被调用,子类的资源没有正确是释放,因此造成内 存泄露。

## 对于 C++源文件,从文本到可执行文件一般需要四个过程:

1. 预处理阶段:对源代码文件中文件包含关系(头文件).预编译语句(宏定义)进行分析和替换,生成预编译文件。 

   主要处理源代码文件中的以"#"开头的预编译指令。处理规则见下 

   1. 删除所有的#define,展开所有的宏定义
   2. 处理所有的条件预编译指令,如"#if"."#endif"."#ifdef"."#elif"和"#else"。
   3. 处理"#include"预编译指令,将文件内容替换到它的位置,这个过程是递归进行的,文件 中包含其他文件。 
   4. 删除所有的注释,"//"和"/**/"。
   5. 保留所有的#pragma 编译器指令,编译器需要用到他们,如:#pragma once 是为了防止有文 件被重复引用。
   6. 添加行号和文件标识,便于编译时编译器产生调试用的行号信息,和编译时产生编译错误或 警告是能够显示行号。

2. 编译阶段:将经过预处理后的预编译文件转换成特定汇编代码,生成汇编文件

   把预编译之后生成的 xxx.i 或 xxx.ii 文件，进行一系列词法分析.语法分析.语义分析及优化 后，生成相应的汇编代码文件。

   1. 词法分析：利用类似于“有限状态机”的算法，将源代码程序输入到扫描机中，将其中的字 符序列分割成一系列的记号。
   2. 语法分析：语法分析器对由扫描器产生的记号，进行语法分析，产生语法树。由语法分析器 输出的语法树是一种以表达式为节点的树。
   3. 语义分析：语法分析器只是完成了对表达式语法层面的分析，语义分析器则对表达式是否有 意义进行判断，其分析的语义是静态语义——在编译期能分期的语义，相对应的动态语义是在运 行期才能确定的语义。
   4. 优化：源代码级别的一个优化过程。
   5. 目标代码生成：由代码生成器将中间代码转换成目标机器代码，生成一系列的代码序列—— 汇编语言表示。
   6. 目标代码优化：目标代码优化器对上述的目标机器代码进行优化：寻找合适的寻址方式.使 用位移来替代乘法运算.删除多余的指令等。

3. 汇编阶段:将编译阶段生成的汇编文件转化成机器码,生成可重定位目标文件 

   将汇编代码转变成机器可以执行的指令(机器码文件)。 汇编器的汇编过程相对于编译器来说更 简单，没有复杂的语法，也没有语义，更不需要做指令优化，只是根据汇编指令和机器指令的对 照表一一翻译过来，汇编过程有汇编器 as 完成。经汇编之后，产生目标文件(与可执行文件格式 几乎一样)xxx.o(Windows 下)、xxx.obj(Linux 下)。

4. 链接阶段:将多个目标文件及所需要的库连接成最终的可执行目标文件

   * 静态链接：
     函数和数据被编译进一个二进制文件。在使用静态库的情况下，在编译链接可执行文件时，链接 器从库中复制这些函数和数据并把它们和应用程序的其它模块组合起来创建最终的可执行文件。
     * 空间浪费：因为每个可执行程序中对所有需要的目标文件都要有一份副本，所以如果多个程序对 同一个目标文件都有依赖，会出现同一个目标文件都在内存存在多个副本； 
     * 更新困难：每当库函数的代码修改了，这个时候就需要重新进行编译链接形成可执行程序。
     * 运行速度快：但是静态链接的优点就是，在可执行程序中已经具备了所有执行程序所需要的任何 东西，在执行的时候运行速度快。 
   * 动态链接：
     动态链接的基本思想是把程序按照模块拆分成各个相对独立部分，在程序运行时才将它们链接在 一起形成一个完整的程序，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文 件。
     * 共享库：就是即使需要每个程序都依赖同一个库，但是该库不会像静态链接那样在内存中存在多 分，副本，而是这多个程序在执行时共享同一份副本；
     * 更新方便：更新时只需要替换原来的目标文件，而无需将所有的程序再重新链接一遍。当程序下 一次运行时，新版本的目标文件会被自动加载到内存并且链接起来，程序就完成了升级的目标。
     * 性能损耗：因为把链接推迟到了程序运行时，所以每次执行程序都需要进行链接，所以性能会有 一定损失。

## include ""和<> 的区别

* 编译器预处理阶段查找头文件的路径不一样。
* ""包含的头文件 查找顺序
* 当前头文件目录 
* 编译器设置的头文件路径(编译器可使用-I 显式指定搜索路径) 
* 系统变量 CPLUS_INCLUDE_PATH/C_INCLUDE_PATH 指定的头文件路径 
* 对于使用尖括号包含的头文件
* 编译器设置的头文件路径(编译器可使用-I 显式指定搜索路径)
* 系统变量 CPLUS_INCLUDE_PATH/C_INCLUDE_PATH 指定的头文件路

## malloc 原理 brk系统调用和mmap系统调用的作用分别是什么

* Malloc 用于动态分配内存 为了减少内存碎片和系统调用的开销 malloc 使用**内存池**的方式 先申请大块内存作为**堆区** 然后将堆区分为多个内存块。一块作为内存管理的基本单位。放用户申请内存时,直接从堆分配一块合石的空闲块。Malloc 采用**隐式链表**将堆区分为连续的 大小不一的块 ,包含已分配块和未分配块 同时采用**显式链表**管理所有**空闲的块**。即使用一个双向链表将空闲块连接起来,每一个空闲块记录了一个连续的未分配的地址。
* 内存分配时 malloc 会通过隐式链表遍历所有空闲的块 选择满足需求的块进行分配
* 内存合并时 采用非边界标记算法 根据每个块的前后快是否分配来决定是否合并
* 申请内存一半会通过brk和mmap系统调用进行申请 小于128K时。会使用系统函数brk再堆区分配,大于128K 时 会使用系统函数mmap在映射区分配

## GDB

GDB 的出现减轻了开发人员的负担，他们可以在程序运行的时候单步跟踪自己的代码，或者通过 断点暂时中止程序的执行。此外，他们还能够随时察看变量和内存的当前状态，并监视关键的数 据结构是如何影响代码运行的。

### 条件断点

条件断点是当满足条件就中断程序运行，命令：break line-or-function if expr。 例如：(gdb)break 666 if testsize==100

## 段错误

* 访问非法地址内存:
* 使用野指针
* 试图修改字符串常量的内容

## new和malloc

| new                                                          | malloc                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据类型进行分配                                             | 指定的大小分配                                               |
| 返回的是指定对象的指针而                                     | 返回的是 void*,因此 malloc 的返回值一般都需要进行类型转化。  |
| new 不仅分配一段内存,而且会调用构造函数                      | 分配的内存要用 delete 销毁,delete 销毁的时候会调用对象的析构函数 |
| new 是一个操作符可以重载                                     | malloc 是一个库函数。                                        |
| new 没用这样操作                                             | malloc 分配的内存不够的时候,可以用 realloc 扩容。。          |
| new 如果分配失败了会抛出 bad_malloc 的异常                   | 而 malloc 失败了会返回 NULL。                                |
| 申请数组时: new[]一次分配所有内存,多次调用构造函数,搭配使用 delete[],delete[]多次调用析构函数,销毁数组中的每个对象 | 而 malloc 则只能 sizeof(int) * n。                           |

## 内存相关API

* Linux 允许不同进程访问同一个逻辑内存,提供了一组 API,头文件在 sys/shm.h 中。 
* 1)新建共享内存 shmget 
  * int shmget(key_t key,size_t size,int shmflg); 
  * key:共享内存键值,可以理解为共享内存的唯一性标记。 
  * size:共享内存大小 shm
  * flag:创建进程和其他进程的读写权限标识。 返回值:相应的共享内存标识符,失败返回-1 
* 2)连接共享内存到当前进程的地址空间 shmat 
  * void *shmat(int shm_id,const void *shm_addr,int shmflg); 
  * shm_id:共享内存标识符 
  * shm_addr:指定共享内存连接到当前进程的地址,通常为 0,表示由系统来选择。 
  * shmflg:标志位 返回值:指向共享内存第一个字节的指针,失败返回-1
  * 当前进程分离共享内存 shmdt 
* int shmdt(const void *shmaddr); 
  * 控制共享内存 shmctl 和信号量的 semctl 函数类似,控制共享内存 
* int shmctl(int shm_id,int command,struct shmid_ds *buf); 
  * shm_id:共享内存标识符 
  * command: 有三个值
  * IPC_STAT:获取共享内存的状态,把共享内存的 shmid_ds 结构复制到 buf 中。
  * IPC_SET:设置共享内存的状态,把 buf 复制到共享内存的 shmid_ds 结构。 
  * IPC_RMID:删除共享内存 buf:共享内存管理结构体。

## STL 内存优化

  * 二级内存配置结构
    * 第一级配置器
      * 以 malloc(),free(),realloc()等 C 函数执行实际的内存配置.释放.重新配置等操作,并且能在内存需求不被满足的时候,调用一个指定的函数。 
      * 一级空间配置器分配的是大于 128 字节的空间 
      * 如果分配不成功,调用句柄释放一部分内存 
      * 如果还不能分配成功,抛出异常
    * 第二级配置器
      * 在 STL 的第二级配置器中多了一些机制,避免太多小区块造成的内存碎片,小额区块带来的不仅是内存碎片,配置时还有额外的负担。区块越小,额外负担所占比例就越大
  * 分配原则 
    * 如果要分配的区块大于 128bytes,则移交给第一级配置器处理。
    * 如果要分配的区块小于 128bytes,则以内存池管理(memory pool),又称之次层配置(sub-allocation):每次配置一大块内存,并维护对应的 16 个空闲链表(free-list)。下次若有相同大小的内存需求,则直接从 free-list 中取。如果有小额区块被释放,则由配置器回收到 free-list 中。
    * 当用户申请的空间小于 128 字节时,将字节数扩展到 8 的倍数,然后在自由链表中查找对应大小的子链表
      * 如果在自由链表查找不到或者块数不够,则向内存池进行申请,一般一次申请 20 块
      * 如果内存池空间足够,则取出内存
      * 如果不够分配 20 块,则分配最多的块数给自由链表,并且更新每次申请的块数
        如果一块都无法提供,则把剩余的内存挂到自由链表,然后向系统 heap 申请空间,如果申请失败,则看看自由链表还有没有可用的块,如果也没有,则最后调用一级空间配置器
    * 二级内存池
      二级内存池采用了16个空闲链表,这里的16个空闲链表分别管理大小为8.16.24......120.128 的数据块。这里空闲链表节点的设计十分巧妙,这里用了一个联合体既可以表示下一个空闲数据块(存在于空闲链表中)的地址,也可以表示已经被用户使用的数据块(不存在空闲链表中)的地址。
      * ![image-20200706134604300](E:/Notes/MarkDown/计算机课程学习/imgs/C++面试_5_3.jpg)
      * 空间配置函数 allocate
        首先先要检查申请空间的大小,如果大于 128 字节就调用第一级配置器,小于 128 字节就检查对应的空闲链表,如果该空闲链表中有可用数据块,则直接拿来用(拿取空闲链表中的第一个可用数据块,然后把该空闲链表的地址设置为该数据块指向的下一个地址),如果没有可用数据块,则调用 refill 重新填充空间。
      * 空间释放函数 deallocate首先先要检查释放数据块的大小,如果大于 128 字节就调用第一级配置器,小于 128 字节则根据数据块的大小来判断回收后的空间会被插入到哪个空闲链表。 
      * 重新填充空闲链表 refill 在用 allocate 配置空间时,如果空闲链表中没有可用数据块,就会调用 refill 来重新填充空间,新的空间取自内存池。缺省取 20 个数据块,如果内存池空间不足,那么能取多少个节 点就取多少个。
        从内存池取空间给空闲链表用是 chunk_alloc 的工作,首先根据 end_free-start_free 来判断内存池中的剩余空间是否足以调出 nobjs 个大小为 size 的数据块出去,如果内存连一个数 据块的空间都无法供应,需要用 malloc 取堆中申请内存。假如山穷水尽,整个系统的堆空间都不够用了,malloc 失败,那么 chunk_alloc 会从空闲链表中找是否有大的数据块,然后将该数据块的空间分给内存池(这个数据块会从链表中去除)。 
    * 总结: 
      * 使用 allocate 向内存池请求 size 大小的内存空间,
        * 如果需要请求的内存大小大于128bytes,直接使用 malloc。 
        * 如果需要的内存大小小于 128bytes,allocate 根据 size 找到最适合的自由链表。 
          * 如果链表不为空,返回第一个 node,链表头改为第二个 node。
          * 如果链表为空,使用 blockAlloc 请求分配 node。
            *  如果内存池中有大于一个node的空间,分配竟可能多的node(但是最多20个),将一个 node 返回,其他的 node 添加到链表中。 
            *  如果内存池只有一个 node 的空间,直接返回给用户。 
            *  若果如果连一个 node 都没有,再次向操作系统请求分配内存。 
               * 分配成功,再次进行 b 过程。
               * 分配失败,循环各个自由链表,寻找空间。 
                 * 找到空间,再次进行过程 b
                 * 找不到空间,抛出异常。
      * 用户调用 deallocate 释放内存空间,如果要求释放的内存空间大于 128bytes,直接调用 free。 
      * 否则按照其大小找到合适的自由链表,并将其插入。

```FLOW
        st=>start: 开始框
        128byte=>condition: 内存大于128bytes
        malloc=>operation: 调用malloc
        fl=>operation: 寻找最合适的list
        isempty=>condition: 链表是否为空
        rn=>operation: 返回第一个node链表头 改为第二个node
        ms=>operation: 使用blockAlloc请求分配node
        isnempty=>condition: 是否有大于等于1node的空间
        iseempty=>condition: 是否有等于1node的空间
        ismore1=>condition: 是否有node的空间
        more1=>operation: 分配尽可能多的node[不超过20] 将一个node返回 其他node 添加到链表中
        equal1=>operation: 直接返回node
        less1=>condition: 一个node都没有 再次向系统申请内存分配
        mallocs=>operation: 分配成功 执行b过程
        mallocf=>condition: 分配失败 循环各个自由链表 寻找空间
        fss=>operation: 找到空间再次分配
        fsf=>operation: 找不到空间抛出异常
        e=>end: 结束框
        st()->128byte
        128byte(yes)->malloc->e
        128byte(no)->fl->isempty
        isempty(yes)->ms->isnempty
        isempty(no)->rn->e
        isnempty(yes)->iseempty
        iseempty(yes)->more1
        iseempty(no)->equal1
        isnempty(no)->less1
        less1(yes)->mallocs
        less1(no)->mallocf
        mallocf(yes)->fss
        mallocf(no)->fsf
```

## IO 模型

* 阻塞 IO:调用者调用了某个函数，等待这个函数返回，期间什么也不做，不停的去检查这个函 数有没有返回，必须等这个函数返回才能进行下一步动作 
* 非阻塞 IO:非阻塞等待，每隔一段时间就去检测 IO 事件是否就绪。没有就绪就可以做其他事。 
* 信号驱动 IO:信号驱动 IO:linux 用套接口进行信号驱动 IO，安装一个信号处理函数，进程继 续运行并不阻塞，当 IO 时间就绪，进程收到 SIGIO 信号。然后处理 IO 事件。 
* IO 复用/多路转接 IO:linux 用 select/poll 函数实现 IO 复用模型，这两个函数也会使进程阻 塞，但是和阻塞 IO 所不同的是这两个函数可以同时阻塞多个 IO 操作。而且可以同时对多个读操 作、写操作的 IO 函数进行检测。知道有数据可读或可写时，才真正调用 IO 操作函数 
* 异步 IO:linux 中，可以调用 aio_read 函数告诉内核描述字缓冲区指针和缓冲区的大小、文件 偏移及通知的方式，然后立即返回，当内核将数据拷贝到缓冲区后，再通知应用程序。

## page cache

加快文件读取速率

page cache 中由一部分磁盘文件的缓存。

读取文件先去page cache 中查找 如果命中就不用去磁盘中读取。提高读取速度。

在 Linux 内核中，文件的每个数据块最多只能对应一个 Page Cache 项，它通过 两个数据结构来管理这些 Cache 项，一个是 radix tree，另一个是双向链表。

Radix tree 是一种搜索树，Linux 内核利用这个数据结构来通过文件内偏移快速定位 Cache 项

## Linux 怎么获得文件的100-200行

```shell
sed -n '100,200p' inputfile 
awk 'NR>=100&&NR<=200{print}' inputfile 
head -200 inputfile|tail -100
```

## awk

* 作用

  样式扫描和处理语言。它允许创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、 对输入执行计算以及生成报表，还有无数其他的功能。

* 用法

```shell
awk [-F field-separator] 'commands' input-file(s)
```

* 内置变量

命令|介绍
---|---
ARGC|命令行参数个数 
ARGV|命令行参数排列 
ENVIRON|支持队列中系统环境变量的使用 
FILENAME|awk 浏览的文件名 
FNR|浏览文件的记录数 
FS|设置输入域分隔符，等价于命令行 -F 选项 
NF|浏览记录的域的个数 
NR|已读的记录数
OFS|输出域分隔符 
ORS|输出记录分隔符 
RS|控制记录分隔符

例子

1、找到当前文件夹下所有的文件和子文件夹,并显示文件大小

```shell
> ls -l | awk '{print $5 "\t" $9}'
```


读入有'\n'换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域。$0 则表示 所有域,$1 表示第一个域,$n 表示第 n 个域。默认域分隔符是"空白键" 或 "[tab]键"。 

2、找到当前文件夹下所有的文件和子文件夹，并显示文件大小，并显示排序

```shell
> ls -l | awk 'BEGIN {COUNT = -1; print "BEGIN COUNT"} {COUNT = COUNT + 1; print COUNT"\t"$5"\t"$9}END {print "END, COUNT = "COUNT}' 
```

先处理 BEGIN， 然后进行文本分析，进行第二个{}的操作，分析完进行 END 操作。 3、找到当前文件夹下所有的子文件夹,并显示排序
ls -l | awk 'BEGIN {print "BEGIN COUNT"} /4096/{print NR"\t"$5"\t"$9} END {print "END"}' * /4096/ 正则匹配式子

* 使用 print $NF 可以打印出一行中的最后一个字段，使用$(NF-1)则是打印倒数第二个字段， 其他以此类推

## Linux内核中Timer定时器机制

### 低精度时钟 

Linux 2.6.16 之前，内核只支持低精度时钟，内核定时器的工作方式： 

* 系统启动后，会读取时钟源设备(RTC, HPET，PIT…)，初始化当前系统时间。
* 内核会根据 HZ(系统定时器频率，节拍率)参数值，设置时钟事件设备，启动 tick(节拍)
  中断。HZ 表示 1 秒种产生多少个时钟硬件中断，tick 就表示连续两个中断的间隔时间。
* 设置时钟事件设备后，时钟事件设备会定时产生一个 tick 中断，触发时钟中断处理函数，更新系统时钟,并检测 timer wheel，进行超时事件的处理。

在上面工作方式下，Linux 2.6.16 之前，内核软件定时器采用 timer wheel 多级时间轮的实现机制，维护操作系统的所有定时事件。timer wheel 的触发是基于系统 tick 周期性中断。
所以说这之前，linux 只能支持 ms 级别的时钟，随着时钟源硬件设备的精度提高和软件高精度计时的需求，有了高精度时钟的内核设计。

### 高精度时钟

Linux 2.6.16 ，内核支持了高精度的时钟，内核采用新的定时器 hrtimer，其实现逻辑和Linux 2.6.16 之前定时器逻辑区别：
hrtimer 采用红黑树进行高精度定时器的管理，而不是时间轮；
高精度时钟定时器不在依赖系统的 tick 中断，而是基于事件触发。
旧内核的定时器实现依赖于系统定时器硬件定期的 tick，基于该 tick，内核会扫描 timer wheel 处理超时事件，会更新 jiffies，wall time(墙上时间，现实时间)，process 的使用时间
等等工作。
新的内核不再会直接支持周期性的 tick，新内核定时器框架采用了基于事件触发，而不是以前的周期性触发。新内核实现了 hrtimer(high resolution timer)：于事件触发。
hrtimer 的工作原理：
通过将高精度时钟硬件的下次中断触发时间设置为红黑树中最早到期的 Timer 的时间，时
钟到期后从红黑树中得到下一个 Timer 的到期时间，并设置硬件，如此循环反复。
在高精度时钟模式下，操作系统内核仍然需要周期性的 tick 中断，以便刷新内核的一些任
务。hrtimer 是基于事件的，不会周期性出发 tick 中断，所以为了实现周期性的 tick 中断
(dynamic tick)：系统创建了一个模拟 tick 时钟的特殊 hrtimer，将其超时时间设置为一个
tick 时长，在超时回来后，完成对应的工作，然后再次设置下一个 tick 的超时时间，以此达到
周期性 tick 中断的需求。
引入了 dynamic tick，是为了能够在使用高精度时钟的同时节约能源，这样会产生 tickless
情况下，会跳过一些 tick。
新内核对相关的时间硬件设备进行了统一的封装，定义了主要有下面两个结构：
时钟源设备(closk source device)：抽象那些能够提供计时功能的系统硬件，比如
RTC(Real Time Clock)、TSC(Time Stamp Counter)，HPET，ACPI PM-Timer，PIT 等。不同时钟
源提供的精度不一样，现在 pc 大都是支持高精度模式(high-resolution mode)也支持低精度模
式(low-resolution mode)。时钟事件设备(clock event device)：系统中可以触发 one-shot（单次）或者周期性中断
的设备都可以作为时钟事件设备。 当前内核同时存在新旧 timer wheel 和 hrtimer 两套 timer 的实现，内核启动后会进行从
低精度模式到高精度时钟模式的切换，hrtimer 模拟的 tick 中断将驱动传统的低精度定时器系 统（基于时间轮）和内核进程调度。

